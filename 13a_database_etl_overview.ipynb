{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fd2c13-70c2-44d9-bafe-ade475a787a9",
   "metadata": {},
   "source": [
    "#### ðŸ“˜ Combined Database & ETL Pipeline â€” SQL Server + PostgreSQL + Azure AI Foundry\n",
    "\n",
    "#### ðŸ“– Title\n",
    "\n",
    "End-to-End ETL Pipeline with SQL Server, PostgreSQL, Python, and Azure AI Foundry\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Œ Purpose\n",
    "\n",
    "This diagram illustrates how your **local databases (SQL Server + PostgreSQL)** integrate with **Python ETL scripts**, and how the processed data flows into **Azure AI Foundry** for advanced analytics and machine learning.\n",
    "\n",
    "It shows the **big-picture workflow** youâ€™ll follow across this project:\n",
    "\n",
    "1. **Extract** data from SQL Server and PostgreSQL.\n",
    "2. **Transform** data inside Python (cleaning, joining, feature engineering).\n",
    "3. **Load** structured datasets into Azure AI Foundry for experimentation, ML, and deployment.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”Ž Components Explained\n",
    "\n",
    "#### ðŸŸ¦ SQL Server (AI)\n",
    "\n",
    "* Installed locally (`Server Name: AI`).\n",
    "* Acts as a **source system**.\n",
    "* Holds transactional/business datasets.\n",
    "* Connected via **ODBC Driver 18** and Python (`pyodbc`, `SQLAlchemy`).\n",
    "\n",
    "#### ðŸŸ¦ PostgreSQL (Local)\n",
    "\n",
    "* Installed locally (PostgreSQL 17).\n",
    "* Acts as a **secondary database** (e.g., staging or analytics).\n",
    "* Useful for testing **cross-database ETL flows**.\n",
    "* Connected via `psycopg2` and `SQLAlchemy`.\n",
    "\n",
    "#### ðŸŸ§ Python ETL Scripts (pandas + SQLAlchemy)\n",
    "\n",
    "* Located in your `scripts/` folder.\n",
    "* **Core of the pipeline**:\n",
    "\n",
    "  * Extracts data from SQL Server & PostgreSQL.\n",
    "  * Cleans and transforms datasets (`pandas`).\n",
    "  * Loads structured data to Azure AI Foundry.\n",
    "* Python makes the pipeline **reproducible** and **automatable**.\n",
    "\n",
    "#### ðŸŸ© Azure AI Foundry (ML Workspace)\n",
    "\n",
    "* Cloud-based **machine learning workspace**.\n",
    "* Receives prepared datasets from Python ETL.\n",
    "* Used for:\n",
    "\n",
    "  * Training machine learning models.\n",
    "  * Running experiments.\n",
    "  * Deploying AI agents.\n",
    "* Integrates seamlessly with Azure Storage and AI models.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”€ Data Flow\n",
    "\n",
    "1. **Extract (from SQL Server + PostgreSQL)**\n",
    "\n",
    "   * Python pulls data via database connectors.\n",
    "   * SQL queries run to fetch raw tables.\n",
    "\n",
    "2. **Transform & Load (inside Python)**\n",
    "\n",
    "   * Python scripts clean, merge, and reshape data.\n",
    "   * Transformed datasets are written into Azure AI Foundryâ€™s workspace.\n",
    "\n",
    "3. **Consume in Azure AI Foundry**\n",
    "\n",
    "   * AI Foundry stores datasets in its **data assets**.\n",
    "   * ML workflows use them for model training and analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059202b-08d2-4a2b-a3b5-644a2b6979d4",
   "metadata": {},
   "source": [
    "### ðŸ“Š Diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81440410-2ec2-4949-8136-f4efa42b1285",
   "metadata": {},
   "source": [
    "<img src=\"img/Combined_Database_And_ETL_Pipeline.png\" alt=\"Description of Image\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d50ab2-c7f9-471b-a394-da021dc2cd14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "âœ… This documentation ensures future-you (or collaborators) instantly understand **how SQL Server, PostgreSQL, Python, and Azure AI Foundry interact** in your project.\n",
    "\n",
    "---\n",
    "\n",
    "**save this as `13a_database_etl_overview.md`** so you have it as a standalone reference (instead of cluttering `13_sqlserver_installation.md`)? This way, you can link back to it from both PostgreSQL and SQL Server docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980b48d-e9bd-48d6-a344-8048283f458b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
